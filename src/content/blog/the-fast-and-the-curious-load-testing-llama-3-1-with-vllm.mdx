---
title: "The fast and the curious load testing Llama 3.1 with vLLM"
description: "My description"
pubDate: 2024-11-05
coverImage: "/f310d0b1-419a-477d-9c44-b6ec703f1c2e.jpg"
---

import ThroughputChartWrapper from "@components/ThroughputChartWrapper.astro";

<ThroughputChartWrapper
  data={[
    {
      users: 1,
      BF16: 48.4,
      FP8: 77.2,
      W4A16: 107.1,
      W8A8: 70.1,
    },
    {
      users: 5,
      BF16: 46.1,
      FP8: 73.1,
      W4A16: 91.7,
      W8A8: 65.5,
    },
    {
      users: 10,
      BF16: 41.8,
      FP8: 59.7,
      W4A16: 82.6,
      W8A8: 62.8,
    },
    {
      users: 25,
      BF16: 36.6,
      FP8: 44.9,
      W4A16: 56.7,
      W8A8: 51.8,
    },
    {
      users: 50,
      BF16: 27.6,
      FP8: 27.9,
      W4A16: 32.3,
      W8A8: 35.0,
    },
    {
      users: 100,
      BF16: 18.2,
      FP8: 14.1,
      W4A16: 17.3,
      W8A8: 20.5,
    },
    {
      users: 250,
      BF16: 5.7,
      FP8: 4.2,
      W4A16: 5.2,
      W8A8: 8.4,
    },
    {
      users: 500,
      BF16: 2.8,
      FP8: 1.8,
      W4A16: 2.1,
      W8A8: 3.5,
    },
    {
      users: 1000,
      BF16: 1.4,
      FP8: 1.1,
      W4A16: 1.3,
      W8A8: 2.5,
    },
  ]}
  caption="Throughput Scaling of Llama 3.1 8B Under Various Quantization Methods on an NVIDIA A6000"
/>

<ThroughputChartWrapper
  data={[
    {
      users: 1,
      BF16: 18.5,
      FP8: 30.5,
      W4A16: 36.9,
      W8A8: 28.9,
    },
    {
      users: 5,
      BF16: 14.6,
      FP8: 22.3,
      W4A16: 24.4,
      W8A8: 21.1,
    },
    {
      users: 10,
      BF16: 11.8,
      FP8: 15.8,
      W4A16: 16.7,
      W8A8: 16.3,
    },
    {
      users: 25,
      BF16: 8.4,
      FP8: 10.0,
      W4A16: 10.3,
      W8A8: 10.2,
    },
    {
      users: 50,
      BF16: 5.3,
      FP8: 5.7,
      W4A16: 5.4,
      W8A8: 6.0,
    },
    {
      users: 100,
      BF16: 2.7,
      FP8: 3.2,
      W4A16: 3.3,
      W8A8: 3.8,
    },
    {
      users: 250,
      BF16: 1.0,
      FP8: 0.9,
      W4A16: 1.0,
      W8A8: 1.1,
    },
  ]}
  caption="Throughput Scaling of Llama 3.1 70B Under Various Quantization Methods on 4 x NVIDIA A6000"
/>
